# Basic Example - Zerobus Rust SDK

This example demonstrates how to use the Zerobus Rust SDK to ingest data into a Databricks Delta table.

## Overview

This example shows:
- Creating a stream with authentication
- Ingesting a single record
- Waiting for acknowledgment
- Properly closing the stream
- How to configure your table into this example

## Prerequisites

### 1. Create a Databricks Table

First, create a table in your Databricks workspace using the following SQL:

```sql
CREATE TABLE catalog.schema.orders (
  id INT,
  customer_name STRING,
  product_name STRING,
  quantity INT,
  price DOUBLE,
  status STRING,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
);
```

Replace `catalog.schema.orders` with your actual catalog, schema, and table name.

### 2. Set Up OAuth Service Principal

You'll need a Databricks service principal with OAuth credentials:

1. In your Databricks workspace, go to **Settings** â†’ **Identity and Access**
2. Create a service principal or use an existing one
3. Generate OAuth credentials (client ID and secret)
4. Grant the service principal the following permissions on your table:
   - `SELECT` - Read table schema
   - `MODIFY` - Write data to the table
   - `USE CATALOG` and `USE SCHEMA` - Access catalog and schema

### 3. Configure Credentials

Edit `src/main.rs` and update the following constants with your actual values:

```rust
const DATABRICKS_WORKSPACE_URL: &str = "https://your-workspace.cloud.databricks.com";
const TABLE_NAME: &str = "catalog.schema.orders";
const DATABRICKS_CLIENT_ID: &str = "your-client-id";
const DATABRICKS_CLIENT_SECRET: &str = "your-client-secret";
const SERVER_ENDPOINT: &str = "https://workspace-id.zerobus.region.cloud.databricks.com";
```

**How to get these values:**
- **DATABRICKS_WORKSPACE_URL** - Your Databricks workspace URL (Unity Catalog endpoint)
- **TABLE_NAME** - Full table name in format `catalog.schema.table`
- **DATABRICKS_CLIENT_ID** - OAuth 2.0 client ID from your service principal
- **DATABRICKS_CLIENT_SECRET** - OAuth 2.0 client secret from your service principal
- **SERVER_ENDPOINT** - Zerobus ingestion endpoint (usually `https://<workspace-id>.zerobus.<region>databricks.com`)

## Running the Example

Once everything is configured:

```bash
cargo run
```

**Expected output:**

```
Record acknowledged with offset Id: 0
Stream closed successfully
```

## Adapting for a Custom Table

To adapt this example for your own table, follow these steps:

### 1. Generate Schema Files

First, use the schema generation tool to create the necessary Rust and descriptor files for your table. Run this from the `zerobus_sdk/` root directory:

```bash
cd ../../tools/generate_files

cargo run -- \
  --uc-endpoint "https://<your-workspace-id>.zerobus.<region>.cloud.databricks.com" \
  --uc-token "<your_pat_token>" \
  --table "<catalog.schema.orders>" \
  --output-dir "examples/basic_example/output"
```

This generates:
- `output/<YOUR_TABLE>.proto` - Protocol Buffer schema
- `output/<YOUR_TABLE>.rs` - Rust structs
- `output/<YOUR_TABLE>.descriptor` - Binary descriptor file

### 2. Update `src/main.rs`

You'll need to make three changes to `src/main.rs` to use your new schema.

**A. Update the `mod` and `use` statements:**
Change `orders` to match the name of your generated Rust file (e.g., `your_table`).

*Before:*
```rust
pub mod orders {
    include!("../output/orders.rs");
}
use crate::orders::TableOrders;
```

*After (for a table named `inventory`):*
```rust
pub mod inventory {
    include!("../output/inventory.rs");
}
use crate::inventory::TableInventory;
```

**B. Update the `load_descriptor_proto` call:**
Change the filenames and message name to match your generated files.

*Before:*
```rust
let descriptor_proto = load_descriptor_proto(
    "output/orders.descriptor",
    "orders.proto",
    "table_Orders"
);
```

*After (for a table named `inventory`):*
```rust
let descriptor_proto = load_descriptor_proto(
    "output/inventory.descriptor",
    "inventory.proto",
    "table_Inventory"
);

**C. Update the record creation:**
Modify the code to create an instance of your new table struct with your own data.

*Before:*
```rust
let ack_future = stream.ingest_record(
    TableOrders {
        id: Some(1),
        customer_name: Some("Alice Smith".to_string()),
        // ... other fields
    }.encode_to_vec()
).await.unwrap();
```

*After (for a table named `inventory`):*
```rust
let ack_future = stream.ingest_record(
    TableInventory {
        item_id: Some(123),
        sku: Some("SKU-XYZ".to_string()),
        // ... other fields
    }.encode_to_vec()
).await.unwrap();
```

### 3. Run the Example
After making these changes, you can run the example:

```bash
cargo run
```

## Code Walkthrough

### 1. Load Descriptor

```rust
let descriptor_proto = load_descriptor_proto(
    "output/orders.descriptor",
    "orders.proto",
    "table_Orders"
);
```

Loads the binary Protocol Buffer descriptor generated by the schema tool.

### 2. Configure Table Properties

```rust
let table_properties = TableProperties {
    table_name: TABLE_NAME.to_string(),
    descriptor_proto,
};
```

Specifies which table to write to and its schema.

### 3. Configure Stream Options

```rust
let stream_configuration_options = StreamConfigurationOptions {
    max_inflight_records: 100,
    ..Default::default()
};
```

Sets the maximum number of inflight records. Default options enable recovery and set reasonable timeouts.

### 4. Initialize SDK

```rust
let sdk_handle = ZerobusSdk::new(
    SERVER_ENDPOINT.to_string(),
    DATABRICKS_WORKSPACE_URL.to_string(),
);
```

Creates the SDK instance with Zerobus and Unity Catalog endpoints.

### 5. Create Stream

```rust
let mut stream = sdk_handle
    .create_stream(
        table_properties,
        DATABRICKS_CLIENT_ID.to_string(),
        DATABRICKS_CLIENT_SECRET.to_string(),
        Some(stream_configuration_options),
    )
    .await
    .expect("Failed to create a stream.");
```

Opens a bidirectional gRPC stream with authentication.

### 6. Ingest Record

```rust
let ack_future = stream
    .ingest_record(
        TableOrders {
            id: Some(1),
            customer_name: Some("Alice Smith".to_string()),
            product_name: Some("Wireless Mouse".to_string()),
            quantity: Some(2),
            price: Some(25.99),
            status: Some("pending".to_string()),
            created_at: Some(chrono::Utc::now().timestamp()),
            updated_at: Some(chrono::Utc::now().timestamp()),
        }
        .encode_to_vec(),
    )
    .await
    .unwrap();
```

Encodes the record as Protocol Buffers and sends it. Returns a future that resolves when the server acknowledges.

### 7. Wait for Acknowledgment

```rust
let _ack = ack_future.await.unwrap();
```

Blocks until the server confirms the record was written.

### 8. Close Stream

```rust
stream.close().await?;
```

Flushes pending records and closes the stream gracefully.

## Customizing the Example

### Change the Record Data

Modify the `TableOrders` struct in step 6 to ingest different data:

```rust
TableOrders {
    id: Some(2),
    customer_name: Some("Bob Jones".to_string()),
    product_name: Some("Keyboard".to_string()),
    quantity: Some(1),
    price: Some(79.99),
    status: Some("shipped".to_string()),
    created_at: Some(chrono::Utc::now().timestamp()),
    updated_at: Some(chrono::Utc::now().timestamp()),
}
```

### Ingest Multiple Records

```rust
for i in 0..100 {
    let ack_future = stream
        .ingest_record(
            TableOrders {
                id: Some(i),
                customer_name: Some(format!("Customer {}", i)),
                // ... other fields
            }
            .encode_to_vec(),
        )
        .await?;

    // Option 1: Fire and forget
    tokio::spawn(ack_future);

    // Option 2: Collect and wait later
    // ack_futures.push(ack_future);
}

// Flush all pending records
stream.flush().await?;
```

## Troubleshooting

### Error: "Failed to create a stream"

**Possible causes:**
- Invalid credentials (client ID or secret)
- Service principal lacks permissions on the table
- Incorrect workspace URL or endpoint
- Table doesn't exist

**Solution:** Verify your credentials and table permissions.

### Error: "Failed to read proto descriptor file"

**Possible causes:**
- Schema files not generated
- Wrong file paths

**Solution:** Run the schema generation tool and verify the `output/` directory contains the generated files.

### Error: "Invalid token"

**Possible causes:**
- OAuth credentials expired or invalid
- Incorrect Unity Catalog endpoint

**Solution:** Regenerate your service principal credentials and verify the endpoint URL.

## Next Steps

- Try ingesting larger batches of records
- Experiment with different `StreamConfigurationOptions`
- Add error handling and retry logic
- Use the SDK in a production application

## Additional Resources

- [Main SDK Documentation](../../README.md)
- [Schema Generation Tool](../../tools/generate_files/readme.md)
- [Databricks Unity Catalog Documentation](https://docs.databricks.com/unity-catalog/index.html)
